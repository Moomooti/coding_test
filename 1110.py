#컴퓨터가 이해할 수 있도록 사람 말을 바꿔주는 방법

'''
텍스트를 단순히 숫자로 맵핑하는 형태로 표현한다면, 문장이 내포하고 있는 의미와 문맥을 제대로 파악하기 어렵다.


Language Model 만들기
(1) 텍스트 전처리 : 컴퓨터가 스스로 학습을 하기 위해서는 데이터를 숫자로 변환해줘야 한다. 
- 단어 임베딩: 텍스트를 숫자로 변환하는 작업
- 텍스트 전처리 = 텍스트 정제 + 단어 임베딩
- 토큰화: 자연어는 대부분 문장으로 구성되어 있습니다. 텍스트 정제의 가장 첫 번째 과정은 긴 문장을 쪼깨는 작업입니다.
- 불용어 제거: 토큰으로 쪼개진 단어 중에서 문장에서 실질적인 의미를 가지고 있지 않은 단어는 분석에서 제외합니다.
- 품사 태깅: 문장에서 하나의 단어가 어떤 상황에서는 명사로 사용되기도 하고, 어떤 상황에서는 형용사로 사용되기도 한다.
            하나의 단어가 여러가지 품사의 역할을 할 수 있기 때문에, 정확하게 어떤 품사인가를 표시하는 작업을 진행하는데 이러한 과정을 품사 태깅이라고 합니다.

텍스트 정제를 위해서 파이썬 라이브러리: nltk, konlpy에서 제공하는 mecab 클래스

(2) 단어 표현
- 원핫 인코딩: 자연어를 분석하기 위해서는 수치형 데이터 즉, 벡터로 변환해야 합니다. 단어를 벡터로 표현하는 방법
             벡터로 변환할 단어의 개수만큼 벡터의 차원을 생성하고, 하나의 값만 1로 설정하고 나머지 데이터는 모두 0을 갖는 인코딩 방식입니다.
- 단어 임베딩: 단어가 가진 의미를 그대로 보존하면서 의미와 맥락을 고려하여 단어를 벡터로 표현하는 것
             Dense Representation이라고 합니다. 대표적인 단어 임베딩 알고리즘은 워드투벡터, 글로브, FastText가 있습니다.
             
             
*기존 RNN구조의 한계와 단어 임베딩 방식을 개선한 인코더-디코더 구조의 언어 모델이 연구됨.

(3) 언어 모델
-인코더: 입력 토큰 임베딩을 받아 시퀀스적 문맥을 요약한 표현을 만든다. 
-디코더: 인코더 표현을 컨텍스트로 참조하여 단계적으로 출력을 생성한다.
-seq2seq: 인코더 LSTM이 입력 전체를 읽어 마지막 은닉 상태에 정보를 압축하고, 디코더 LSTM이 그 상태에서 시작해 한 토큰씩 생성한다.
-어텐션: Q-K-V로 표현된 항들 간 유사도를 바탕으로 가중합을 계산해, 필요한 위치의 정보를 동적을 참조한다.  
마스킹? 디코더 자기언텐션은 미래 토큰을 가려 시퀀스 생성의 인과성을 보장한다.
-트랜스포머: 인코더 스택은 층마다 자기 어텐션+위치별 FFN으로 구성 / 디코더 스택은 마스크드 자기어텐션+인코더-디코더 어텐션+FFN으로 구성
-PLM: 대규모 코퍼스로 사전학습 후, 다운 스트림 태스크에 파인튜닝하거나 프롬프트로 전이 하는 패러다임을 말한다. 
-BERT: 트랜스포머 인코더만 사용하는 양방향 모델로, 마스크드 언어모델과 NSP로 사전학습하여 문맥을 깊게 통합한다.

 
 + 대규모 코퍼스: 거대한 텍스트 데이터 집합
 + 마스크: 가리다, 숨기다 라는 뜻으로, BERT처럼 언어 모델을 학습 시킬 때, 훈련용 문장의 일부 단어를 일부러 가려서 모델이 그 빈칸에 맞는 단어를 추측하는 연습을 
 하는데 이때, 가려진 단어를 나타내기 위해서 MASK라는 특수 토큰을 사용
 
'''
